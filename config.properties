# TorchServe configuration file

# Inference API port
inference_address=http://0.0.0.0:8080

# Management API port
management_address=http://0.0.0.0:8081

# Metrics API port
metrics_address=http://0.0.0.0:8082

# Number of netty threads
num_netty_threads=1

# Number of workers
num_workers=1

# Maximum request size (in bytes)
max_request_size=6553500

# Maximum response size (in bytes)
max_response_size=6553500

# Default workers per model
default_workers_per_model=1

# Model store
model_store=/home/model-server/model-store

